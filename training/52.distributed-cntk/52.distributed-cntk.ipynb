{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 52. Distributed CNTK\n",
    "In this tutorial we demonstrate how to use the Azure ML Training SDK to train CNTK model in a distributed manner.\n",
    "\n",
    "# Prerequisites\n",
    "\n",
    "Make sure you go through the [00. Installation and Configuration](00.configuration.ipynb) Notebook first if you haven't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from azureml.core.experiment import Experiment\n",
    "\n",
    "username = getpass.getuser().replace('-','')\n",
    "\n",
    "# choose a name for the run history container in the workspace\n",
    "run_history_name = username + '-cntk-distrib'\n",
    "\n",
    "experiment = Experiment(ws, run_history_name)\n",
    "\n",
    "# project folder name\n",
    "project_folder = './' + run_history_name\n",
    "\n",
    "print(project_folder)\n",
    "os.makedirs(project_folder, exist_ok = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This recipe is using a MLC-managed Batch AI cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import BatchAiCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "\n",
    "batchai_cluster_name='gpucluster'\n",
    "\n",
    "\n",
    "try:\n",
    "    # Check for existing cluster\n",
    "    compute_target = ComputeTarget(ws,batchai_cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except:\n",
    "    # Else, create new one\n",
    "    print('Creating a new compute target...')\n",
    "    provisioning_config = BatchAiCompute.provisioning_configuration(vm_size = \"STANDARD_NC6\", # NC6 is GPU-enabled\n",
    "                                                                    #vm_priority = 'lowpriority', # optional\n",
    "                                                                    autoscale_enabled = True,\n",
    "                                                                    cluster_min_nodes = 0, \n",
    "                                                                    cluster_max_nodes = 4)\n",
    "    compute_target = ComputeTarget.create(ws, batchai_cluster_name, provisioning_config)\n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    " # For a more detailed view of current BatchAI cluster status, use the 'status' property    \n",
    "print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {project_folder}/cntk_mnist.py\n",
    "\n",
    "# This code is adapted from CNTK MNIST tutorials: \n",
    "# 1. https://github.com/Microsoft/CNTK/blob/v2.0/Tutorials/CNTK_103A_MNIST_DataLoader.ipynb\n",
    "# 2. https://github.com/Microsoft/CNTK/blob/v2.0/Tutorials/CNTK_103C_MNIST_MultiLayerPerceptron.ipynb\n",
    "\n",
    "# Import the relevant modules to be used later\n",
    "from __future__ import print_function\n",
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import struct\n",
    "import sys\n",
    "import time\n",
    "import pandas \n",
    "\n",
    "import cntk as C\n",
    "from azureml.core.run import Run\n",
    "import argparse\n",
    "\n",
    "run = Run.get_submitted_run()\n",
    "\n",
    "parser=argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--learning_rate', type=float, default=0.001, help='learning rate')\n",
    "parser.add_argument('--num_hidden_layers', type=int, default=2, help='number of hidden layers')\n",
    "parser.add_argument('--minibatch_size', type=int, default=64, help='minibatchsize')\n",
    "\n",
    "args=parser.parse_args() \n",
    "\n",
    "# Functions to load MNIST images and unpack into train and test set.\n",
    "# - loadData reads image data and formats into a 28x28 long array\n",
    "# - loadLabels reads the corresponding labels data, 1 for each image\n",
    "# - load packs the downloaded image and labels data into a combined format to be read later by \n",
    "#   CNTK text reader \n",
    "def loadData(src, cimg):\n",
    "    print ('Downloading ' + src)\n",
    "    gzfname, h = urlretrieve(src, './delete.me')\n",
    "    print ('Done.')\n",
    "    try:\n",
    "        with gzip.open(gzfname) as gz:\n",
    "            n = struct.unpack('I', gz.read(4))\n",
    "            # Read magic number.\n",
    "            if n[0] != 0x3080000:\n",
    "                raise Exception('Invalid file: unexpected magic number.')\n",
    "            # Read number of entries.\n",
    "            n = struct.unpack('>I', gz.read(4))[0]\n",
    "            if n != cimg:\n",
    "                raise Exception('Invalid file: expected {0} entries.'.format(cimg))\n",
    "            crow = struct.unpack('>I', gz.read(4))[0]\n",
    "            ccol = struct.unpack('>I', gz.read(4))[0]\n",
    "            if crow != 28 or ccol != 28:\n",
    "                raise Exception('Invalid file: expected 28 rows/cols per image.')\n",
    "            # Read data.\n",
    "            res = np.fromstring(gz.read(cimg * crow * ccol), dtype = np.uint8)\n",
    "    finally:\n",
    "        os.remove(gzfname)\n",
    "    return res.reshape((cimg, crow * ccol))\n",
    "\n",
    "def loadLabels(src, cimg):\n",
    "    print ('Downloading ' + src)\n",
    "    gzfname, h = urlretrieve(src, './delete.me')\n",
    "    print ('Done.')\n",
    "    try:\n",
    "        with gzip.open(gzfname) as gz:\n",
    "            n = struct.unpack('I', gz.read(4))\n",
    "            # Read magic number.\n",
    "            if n[0] != 0x1080000:\n",
    "                raise Exception('Invalid file: unexpected magic number.')\n",
    "            # Read number of entries.\n",
    "            n = struct.unpack('>I', gz.read(4))\n",
    "            if n[0] != cimg:\n",
    "                raise Exception('Invalid file: expected {0} rows.'.format(cimg))\n",
    "            # Read labels.\n",
    "            res = np.fromstring(gz.read(cimg), dtype = np.uint8)\n",
    "    finally:\n",
    "        os.remove(gzfname)\n",
    "    return res.reshape((cimg, 1))\n",
    "\n",
    "def try_download(dataSrc, labelsSrc, cimg):\n",
    "    data = loadData(dataSrc, cimg)\n",
    "    labels = loadLabels(labelsSrc, cimg)\n",
    "    return np.hstack((data, labels))\n",
    "\n",
    "# Save the data files into a format compatible with CNTK text reader\n",
    "def savetxt(filename, ndarray):\n",
    "    dir = os.path.dirname(filename)\n",
    "\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "    if not os.path.isfile(filename):\n",
    "        print(\"Saving\", filename )\n",
    "        with open(filename, 'w') as f:\n",
    "            labels = list(map(' '.join, np.eye(10, dtype=np.uint).astype(str)))\n",
    "            for row in ndarray:\n",
    "                row_str = row.astype(str)\n",
    "                label_str = labels[row[-1]]\n",
    "                feature_str = ' '.join(row_str[:-1])\n",
    "                f.write('|labels {} |features {}\\n'.format(label_str, feature_str))\n",
    "    else:\n",
    "        print(\"File already exists\", filename)\n",
    "\n",
    "# Read a CTF formatted text (as mentioned above) using the CTF deserializer from a file\n",
    "def create_reader(path, is_training, input_dim, num_label_classes):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "        labels = C.io.StreamDef(field='labels', shape=num_label_classes, is_sparse=False),\n",
    "        features   = C.io.StreamDef(field='features', shape=input_dim, is_sparse=False)\n",
    "    )), randomize = is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)\n",
    "\n",
    "# Defines a utility that prints the training progress\n",
    "def print_training_progress(trainer, mb, frequency, verbose=1):\n",
    "    training_loss = \"NA\"\n",
    "    eval_error = \"NA\"\n",
    "\n",
    "    if mb%frequency == 0:\n",
    "        training_loss = trainer.previous_minibatch_loss_average\n",
    "        eval_error = trainer.previous_minibatch_evaluation_average\n",
    "        if verbose: \n",
    "            print (\"Minibatch: {0}, Loss: {1:.4f}, Error: {2:.2f}%\".format(mb, training_loss, eval_error*100))\n",
    "        \n",
    "    return mb, training_loss, eval_error\n",
    "\n",
    "# Create the network architecture\n",
    "def create_model(features):\n",
    "    with C.layers.default_options(init = C.layers.glorot_uniform(), activation = C.ops.relu):\n",
    "            h = features\n",
    "            for _ in range(num_hidden_layers):\n",
    "                h = C.layers.Dense(hidden_layers_dim)(h)\n",
    "            r = C.layers.Dense(num_output_classes, activation = None)(h)\n",
    "            return r\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run = Run.get_submitted_run()\n",
    "\n",
    "    try: \n",
    "        from urllib.request import urlretrieve \n",
    "    except ImportError: \n",
    "        from urllib import urlretrieve\n",
    "\n",
    "    # Select the right target device when this script is being used:\n",
    "    if 'TEST_DEVICE' in os.environ:\n",
    "        if os.environ['TEST_DEVICE'] == 'cpu':\n",
    "            C.device.try_set_default_device(C.device.cpu())\n",
    "        else:\n",
    "            C.device.try_set_default_device(C.device.gpu(0))\n",
    "\n",
    "    # URLs for the train image and labels data\n",
    "    url_train_image = 'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz'\n",
    "    url_train_labels = 'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz'\n",
    "    num_train_samples = 60000\n",
    "\n",
    "    print(\"Downloading train data\")\n",
    "    train = try_download(url_train_image, url_train_labels, num_train_samples)\n",
    "\n",
    "    url_test_image = 'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz'\n",
    "    url_test_labels = 'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'\n",
    "    num_test_samples = 10000\n",
    "\n",
    "    print(\"Downloading test data\")\n",
    "    test = try_download(url_test_image, url_test_labels, num_test_samples)\n",
    "\n",
    "\n",
    "    # Save the train and test files (prefer our default path for the data\n",
    "    rank = os.environ.get(\"OMPI_COMM_WORLD_RANK\")    \n",
    "    data_dir = os.path.join(\"outputs\", \"MNIST\")\n",
    "    sentinel_path = os.path.join(data_dir, \"complete.txt\") \n",
    "    if rank == '0': \n",
    "        print ('Writing train text file...')\n",
    "        savetxt(os.path.join(data_dir, \"Train-28x28_cntk_text.txt\"), train)\n",
    "\n",
    "        print ('Writing test text file...')\n",
    "        savetxt(os.path.join(data_dir, \"Test-28x28_cntk_text.txt\"), test)\n",
    "        with open(sentinel_path, 'w+') as f:\n",
    "          f.write(\"download complete\")\n",
    "\n",
    "        print('Done with downloading data.')\n",
    "    else:\n",
    "        while not os.path.exists(sentinel_path):\n",
    "          time.sleep(0.01)\n",
    "        \n",
    "\n",
    "    # Ensure we always get the same amount of randomness\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Define the data dimensions\n",
    "    input_dim = 784\n",
    "    num_output_classes = 10\n",
    "\n",
    "    # Ensure the training and test data is generated and available for this tutorial.\n",
    "    # We search in two locations in the toolkit for the cached MNIST data set.\n",
    "    data_found = False\n",
    "    for data_dir in [os.path.join(\"..\", \"Examples\", \"Image\", \"DataSets\", \"MNIST\"),\n",
    "                    os.path.join(\"data_\" + str(rank), \"MNIST\"),\n",
    "                    os.path.join(\"outputs\", \"MNIST\")]:\n",
    "        train_file = os.path.join(data_dir, \"Train-28x28_cntk_text.txt\")\n",
    "        test_file = os.path.join(data_dir, \"Test-28x28_cntk_text.txt\")\n",
    "        if os.path.isfile(train_file) and os.path.isfile(test_file):\n",
    "            data_found = True\n",
    "            break\n",
    "    if not data_found:\n",
    "        raise ValueError(\"Please generate the data by completing CNTK 103 Part A\")\n",
    "    print(\"Data directory is {0}\".format(data_dir))\n",
    "\n",
    "    num_hidden_layers = args.num_hidden_layers\n",
    "    hidden_layers_dim = 400\n",
    "\n",
    "    input = C.input_variable(input_dim)\n",
    "    label = C.input_variable(num_output_classes)\n",
    "\n",
    "            \n",
    "    z = create_model(input)\n",
    "    # Scale the input to 0-1 range by dividing each pixel by 255.\n",
    "    z = create_model(input/255.0)\n",
    "\n",
    "    loss = C.cross_entropy_with_softmax(z, label)\n",
    "    label_error = C.classification_error(z, label)\n",
    "\n",
    "\n",
    "    # Instantiate the trainer object to drive the model training\n",
    "    learning_rate = args.learning_rate\n",
    "    lr_schedule = C.learning_rate_schedule(learning_rate, C.UnitType.minibatch)\n",
    "    learner = C.sgd(z.parameters, lr_schedule)\n",
    "    trainer = C.Trainer(z, (loss, label_error), [learner])\n",
    "\n",
    "\n",
    "    # Initialize the parameters for the trainer\n",
    "    minibatch_size = args.minibatch_size\n",
    "    num_samples_per_sweep = 60000\n",
    "    num_sweeps_to_train_with = 10\n",
    "    num_minibatches_to_train = (num_samples_per_sweep * num_sweeps_to_train_with) / minibatch_size\n",
    "\n",
    "    # Create the reader to training data set\n",
    "    reader_train = create_reader(train_file, True, input_dim, num_output_classes)\n",
    "\n",
    "    # Map the data streams to the input and labels.\n",
    "    input_map = {\n",
    "        label  : reader_train.streams.labels,\n",
    "        input  : reader_train.streams.features\n",
    "    } \n",
    "\n",
    "    # Run the trainer on and perform model training\n",
    "    training_progress_output_freq = 500\n",
    "    \n",
    "    errors = []\n",
    "    losses = []\n",
    "    for i in range(0, int(num_minibatches_to_train)):        \n",
    "        # Read a mini batch from the training data file\n",
    "        data = reader_train.next_minibatch(minibatch_size, input_map = input_map)\n",
    "        \n",
    "        trainer.train_minibatch(data)\n",
    "        batchsize, loss, error = print_training_progress(trainer, i, training_progress_output_freq, verbose=1)\n",
    "        if (error != 'NA') and (loss != 'NA'):\n",
    "            errors.append(float(error))\n",
    "            losses.append(float(loss))\n",
    "    \n",
    "    # log the losses\n",
    "    if rank == '0': \n",
    "        run.log_list(\"Loss\", losses)\n",
    "        run.log_list(\"Error\",errors)\n",
    "\n",
    "    # Read the training data\n",
    "    reader_test = create_reader(test_file, False, input_dim, num_output_classes)\n",
    "\n",
    "    test_input_map = {\n",
    "        label  : reader_test.streams.labels,\n",
    "        input  : reader_test.streams.features,\n",
    "    }\n",
    "\n",
    "    # Test data for trained model\n",
    "    test_minibatch_size = 512\n",
    "    num_samples = 10000\n",
    "    num_minibatches_to_test = num_samples // test_minibatch_size\n",
    "    test_result = 0.0\n",
    "\n",
    "    \n",
    "    for i in range(num_minibatches_to_test):    \n",
    "        # We are loading test data in batches specified by test_minibatch_size\n",
    "        # Each data point in the minibatch is a MNIST digit image of 784 dimensions \n",
    "        # with one pixel per dimension that we will encode / decode with the \n",
    "        # trained model.\n",
    "        data = reader_test.next_minibatch(test_minibatch_size,\n",
    "                                        input_map = test_input_map)\n",
    "\n",
    "        eval_error = trainer.test_minibatch(data)\n",
    "        test_result = test_result + eval_error\n",
    "    \n",
    "\n",
    "    # Average of evaluation errors of all test minibatches\n",
    "    print(\"Average test error: {0:.2f}%\".format(test_result*100 / num_minibatches_to_test))\n",
    "\n",
    "    out = C.softmax(z)\n",
    "\n",
    "    # Read the data for evaluation\n",
    "    reader_eval = create_reader(test_file, False, input_dim, num_output_classes)\n",
    "\n",
    "    eval_minibatch_size = 25\n",
    "    eval_input_map = {input: reader_eval.streams.features} \n",
    "\n",
    "    data = reader_test.next_minibatch(eval_minibatch_size, input_map = test_input_map)\n",
    "\n",
    "    img_label = data[label].asarray()\n",
    "    img_data = data[input].asarray()\n",
    "    predicted_label_prob = [out.eval(img_data[i]) for i in range(len(img_data))]\n",
    "\n",
    "    # Find the index with the maximum value for both predicted as well as the ground truth\n",
    "    pred = [np.argmax(predicted_label_prob[i]) for i in range(len(predicted_label_prob))]\n",
    "    gtlabel = [np.argmax(img_label[i]) for i in range(len(img_label))]\n",
    "\n",
    "    print(\"Label    :\", gtlabel[:25])\n",
    "    print(\"Predicted:\", pred)\n",
    "    \n",
    "    # save model to outputs folder\n",
    "    z.save('outputs/cntk.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import *\n",
    "pip_packages=['cntk==2.5.1', 'pandas==0.23.4']\n",
    "cntk_estimator = Estimator(source_directory=project_folder,\n",
    "                      compute_target=compute_target,\n",
    "                      entry_script='cntk_mnist.py',\n",
    "                      node_count=2,\n",
    "                      process_count_per_node=1,\n",
    "                      distributed_backend=\"mpi\",     \n",
    "                      pip_packages=pip_packages,\n",
    "                      custom_docker_base_image=\"microsoft/mmlspark:0.12\",\n",
    "                      use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(cntk_estimator)\n",
    "print(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
